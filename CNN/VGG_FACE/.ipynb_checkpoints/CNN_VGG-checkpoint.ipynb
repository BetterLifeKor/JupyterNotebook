{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import image_from_tfrecord as read_data\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOAD_MODEL = False\n",
    "START_ITER = 0\n",
    "model_path = \"./model-0000.ckpt\"\n",
    "\n",
    "# Define Prameters\n",
    "DATA_TYPE = tf.float32\n",
    "SEED = 4658\n",
    "\n",
    "NUM_QUEUE_THREAD = 4\n",
    "\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_VALIDATION = 25\n",
    "\n",
    "IMAGE_HEIGHT = 180\n",
    "IMAGE_WIDTH = 140\n",
    "NUM_CHANNEL = 3\n",
    "\n",
    "NUM_TRAIN_CASE = 193515\n",
    "NUM_VALIDATION_CASE = 48379\n",
    "NUM_CLASS = 8277\n",
    "\n",
    "WEIGHT_DECAY_FACTOR = 0.0005\n",
    "MOMENTUM_FACTOR = 0.9\n",
    "\n",
    "NUM_EPOCH = 30\n",
    "SAVE_FREQ = 5000\n",
    "TEST_FREQ = 1000\n",
    "DISP_FREQ = 10\n",
    "MAX_ITER = 100 # 400000\n",
    "\n",
    "MAX_ITER_TEST = NUM_VALIDATION_CASE // BATCH_SIZE_VALIDATION\n",
    "\n",
    "BASE_LR = 0.007\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=WEIGHT_DECAY_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Define Weight Variables\n",
    "# with tf.variable_scope(\"layer1\"):\n",
    "#     conv11_weight =  tf.Variable(\n",
    "#         tf.random_normal([3, 3, NUM_CHANNEL, 64],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='conv1')\n",
    "#     conv11_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[64],\n",
    "#                     dtype=DATA_TYPE), name='bias1')\n",
    "#     conv12_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 64, 64],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight2')\n",
    "#     conv12_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[64],\n",
    "#                     dtype=DATA_TYPE), name='bias2')\n",
    "# with tf.variable_scope(\"layer2\"):\n",
    "#     conv21_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 64, 128],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight1')\n",
    "#     conv21_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[128],\n",
    "#                     dtype=DATA_TYPE), name='bias1')\n",
    "#     conv22_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 128, 128],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight2')\n",
    "#     conv22_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[128],\n",
    "#                     dtype=DATA_TYPE), name='bias2')\n",
    "# with tf.variable_scope(\"layer3\"):\n",
    "#     conv31_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 128, 256],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight1')\n",
    "#     conv31_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[256],\n",
    "#                     dtype=DATA_TYPE), name='bias1')\n",
    "#     conv32_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 256, 256],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight2')\n",
    "#     conv32_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[256],\n",
    "#                     dtype=DATA_TYPE), name='bias2')\n",
    "# with tf.variable_scope(\"layer4\"):\n",
    "#     conv41_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 256, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight1')\n",
    "#     conv41_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias1')\n",
    "#     conv42_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 512, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight2')\n",
    "#     conv42_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias2')\n",
    "#     conv43_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 512, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight3')\n",
    "#     conv43_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias3')\n",
    "# with tf.variable_scope(\"layer5\"):\n",
    "#     conv51_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 512, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight1')\n",
    "#     conv51_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias1')\n",
    "#     conv52_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 512, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight2')\n",
    "#     conv52_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias2')\n",
    "#     conv53_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 512, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight3')\n",
    "#     conv53_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias3')\n",
    "# with tf.variable_scope(\"layer6\"):\n",
    "#     fc6_weight = tf.Variable(\n",
    "#         tf.random_normal([10240, 4096],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight')\n",
    "#     fc6_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[4096],\n",
    "#                     dtype=DATA_TYPE), name='bias')\n",
    "# with tf.variable_scope(\"layer7\"):\n",
    "#     fc7_weight = tf.Variable(\n",
    "#         tf.random_normal([4096, 4096],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight')\n",
    "#     fc7_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[4096],\n",
    "#                     dtype=DATA_TYPE), name='bias')\n",
    "# with tf.variable_scope(\"layer8\"):\n",
    "#     fc8_weight = tf.Variable(\n",
    "#         tf.random_normal([4096, NUM_CLASS + 1],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight')\n",
    "#     fc8_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[NUM_CLASS + 1],\n",
    "#                     dtype=DATA_TYPE), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"layer1\"):\n",
    "    conv11_weight = tf.get_variable(\"weight1\", shape=[3, 3, NUM_CHANNEL, 64],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv11_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[64],\n",
    "                    dtype=DATA_TYPE), name='bias1')\n",
    "    conv12_weight = tf.get_variable(\"weight2\", shape=[3, 3, 64, 64],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv12_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[64],\n",
    "                    dtype=DATA_TYPE), name='bias2')\n",
    "with tf.variable_scope(\"layer2\"):\n",
    "    conv21_weight = tf.get_variable(\"weight1\", shape=[3, 3, 64, 128],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv21_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[128],\n",
    "                    dtype=DATA_TYPE), name='bias1')\n",
    "    conv22_weight = tf.get_variable(\"weight2\", shape=[3, 3, 128, 128],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv22_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[128],\n",
    "                    dtype=DATA_TYPE), name='bias2')\n",
    "with tf.variable_scope(\"layer3\"):\n",
    "    conv31_weight = tf.get_variable(\"weight1\", shape=[3, 3, 128, 256],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv31_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[256],\n",
    "                    dtype=DATA_TYPE), name='bias1')\n",
    "    conv32_weight = tf.get_variable(\"weight2\", shape=[3, 3, 256, 256],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv32_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[256],\n",
    "                    dtype=DATA_TYPE), name='bias2')\n",
    "with tf.variable_scope(\"layer4\"):\n",
    "    conv41_weight = tf.get_variable(\"weight1\", shape=[3, 3, 256, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv41_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias1')\n",
    "    conv42_weight = tf.get_variable(\"weight2\", shape=[3, 3, 512, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv42_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias2')\n",
    "    conv43_weight = tf.get_variable(\"weight3\", shape=[3, 3, 512, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv43_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias3')\n",
    "with tf.variable_scope(\"layer5\"):\n",
    "    conv51_weight = tf.get_variable(\"weight1\", shape=[3, 3, 512, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv51_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias1')\n",
    "    conv52_weight = tf.get_variable(\"weight2\", shape=[3, 3, 512, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED), dtype=DATA_TYPE)\n",
    "    conv52_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias2')\n",
    "    conv53_weight = tf.get_variable(\"weight3\", shape=[3, 3, 512, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv53_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias3')\n",
    "with tf.variable_scope(\"layer6\"):\n",
    "    fc6_weight = tf.get_variable(\"weight\", shape=[10240, 4096],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    fc6_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[4096],\n",
    "                    dtype=DATA_TYPE), name='bias')\n",
    "with tf.variable_scope(\"layer7\"):\n",
    "    fc7_weight = tf.get_variable(\"weight\", shape=[4096, 4096],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    fc7_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[4096],\n",
    "                    dtype=DATA_TYPE), name='bias')\n",
    "with tf.variable_scope(\"layer8\"):\n",
    "    fc8_weight = tf.get_variable(\"weight\", shape=[4096, NUM_CLASS + 1],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    fc8_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[NUM_CLASS + 1],\n",
    "                    dtype=DATA_TYPE), name='bias')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define Deep Model\n",
    "def model(input, dropout_prop):\n",
    "    #Layer1\n",
    "    conv = tf.nn.conv2d(input, conv11_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv11_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv12_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv12_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    # Layer2\n",
    "    conv = tf.nn.conv2d(pool, conv21_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv21_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv22_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv22_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    # Layer3\n",
    "    conv = tf.nn.conv2d(pool, conv31_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv31_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv32_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv32_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    # Layer4\n",
    "    conv = tf.nn.conv2d(pool, conv41_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv41_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv42_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv42_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv43_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv43_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    # Layer5\n",
    "    conv = tf.nn.conv2d(pool, conv51_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv51_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv52_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv52_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv53_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv53_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    #Layer6\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    pool_reshape = tf.reshape(pool, [-1, pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "    ip = tf.matmul(pool_reshape, fc6_weight)\n",
    "    ip = ip + fc6_bias\n",
    "    relu = tf.nn.relu(ip)\n",
    "    do = tf.nn.dropout(relu, dropout_prop, seed=SEED)\n",
    "    # Layer7\n",
    "    ip = tf.matmul(do, fc7_weight)\n",
    "    ip = ip + fc7_bias\n",
    "    relu = tf.nn.relu(ip)\n",
    "    do = tf.nn.dropout(relu, dropout_prop, seed=SEED)\n",
    "    # Layer7\n",
    "    ip = tf.matmul(do, fc8_weight)\n",
    "    ip = ip + fc8_bias\n",
    "\n",
    "    return ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 180, 140, 3)\n",
      "(25, 180, 140, 3)\n"
     ]
    }
   ],
   "source": [
    "tfrecords_filename_train = '../../../DB/data/train_mirror.tfrecords'\n",
    "filename_queue_train = tf.train.string_input_producer([tfrecords_filename_train])\n",
    "image_train, annotation_train = read_data.read_and_decode(filename_queue_train, IMAGE_HEIGHT, IMAGE_WIDTH,\n",
    "                                                          NUM_CHANNEL, NUM_CLASS, BATCH_SIZE_TRAIN, NUM_QUEUE_THREAD)\n",
    "\n",
    "tfrecords_filename_test = '../../../DB/data/validation.tfrecords'\n",
    "filename_queue_test = tf.train.string_input_producer([tfrecords_filename_test])\n",
    "image_test, annotation_test = read_data.read_and_decode(filename_queue_test, IMAGE_HEIGHT, IMAGE_WIDTH,\n",
    "                                                        NUM_CHANNEL, NUM_CLASS, BATCH_SIZE_VALIDATION, NUM_QUEUE_THREAD)\n",
    "\n",
    "print(image_train.shape)\n",
    "print(image_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss\n",
    "data = tf.placeholder(\n",
    "      DATA_TYPE,\n",
    "      [None, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNEL])\n",
    "label = tf.placeholder(tf.int32, [None, NUM_CLASS + 1])\n",
    "do_prop = tf.placeholder(tf.float32)\n",
    "logit = model(data, do_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"weight1:0\", shape=(3, 3, 3, 64), dtype=float32_ref) must be from the same graph as Tensor(\"Placeholder_3:0\", shape=(?, 180, 140, 3), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-4ef7ee4e3d01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_CLASS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdo_prop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlogit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_prop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-d499b59999d6>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(input, dropout_prop)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_prop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#Layer1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv11_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"SAME\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv11_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mrelu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Bob\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m    404\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Bob\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    329\u001b[0m       \u001b[1;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m       \u001b[1;31m# pyline: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Bob\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   3910\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3911\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3912\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3913\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3914\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mC:\\Users\\Bob\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   3849\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m     raise ValueError(\n\u001b[1;32m-> 3851\u001b[1;33m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[0;32m   3852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"weight1:0\", shape=(3, 3, 3, 64), dtype=float32_ref) must be from the same graph as Tensor(\"Placeholder_3:0\", shape=(?, 180, 140, 3), dtype=float32)."
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logit))\n",
    "# Weight decay term\n",
    "reg_ws = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss += tf.reduce_sum(reg_ws)\n",
    "# variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "# for variable in variables:\n",
    "#     loss += WEIGHT_DECAY_FACTOR * tf.sqrt(tf.reduce_sum(tf.square(variable)))\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "global_step = tf.Variable(0, dtype=DATA_TYPE)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.polynomial_decay(\n",
    "    learning_rate=BASE_LR,\n",
    "    global_step=global_step,\n",
    "    decay_steps=MAX_ITER * 2,\n",
    "    end_learning_rate=0.0,\n",
    "    power=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train operator\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=MOMENTUM_FACTOR)\\\n",
    "    .minimize(loss, global_step=global_step)\n",
    "\n",
    "# Calculate accuracy\n",
    "prediction = tf.argmax(tf.nn.softmax(logit), 1)\n",
    "train_label = tf.argmax(label, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, train_label), tf.float32))\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "# init_op = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "total_mean = np.zeros([1, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNEL], dtype=np.float32)\n",
    "with open(\"image_mean.txt\", \"r\") as fd:\n",
    "    image_list = fd.read()\n",
    "    image_list = image_list.split(\"\\n\")\n",
    "    idx = 0\n",
    "    for i in range(IMAGE_HEIGHT):\n",
    "        for j in range(IMAGE_WIDTH):\n",
    "            for k in range(NUM_CHANNEL):\n",
    "                total_mean[0][i][j][k] = image_list[idx]\n",
    "                idx += 1\n",
    "total_mean_repeat_train = np.repeat(total_mean, BATCH_SIZE_TRAIN, axis=0)\n",
    "total_mean_repeat_test = np.repeat(total_mean, BATCH_SIZE_VALIDATION, axis=0)\n",
    "print(total_mean_repeat_train.shape)\n",
    "print(total_mean_repeat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        saver.restore(sess, model_path)\n",
    "    else:\n",
    "        sess.run(init_op)\n",
    "\n",
    "    for train_iter in range(START_ITER, MAX_ITER):\n",
    "        # time_s = time.time()\n",
    "        images_train, annotations_train = sess.run([image_train, annotation_train])\n",
    "        images_train -= total_mean_repeat_train\n",
    "        # time_e = time.time()\n",
    "        # print(\"elapsed time for load image =\", time_e - time_s, \"(s)\")\n",
    "\n",
    "        time_s = time.time()\n",
    "        if train_iter % DISP_FREQ == 0:\n",
    "            #########################Disply Train Info.#################################\n",
    "            print(\"iter\", train_iter)\n",
    "            _, loss_value, acc_value, learning_rate_value = \\\n",
    "                sess.run([optimizer, loss, accuracy, learning_rate],\n",
    "                         feed_dict={data: images_train, label: annotations_train, do_prop: 0.5})\n",
    "            time_e = time.time()\n",
    "\n",
    "            # writer.add_summary(summary, global_step=train_iter)\n",
    "\n",
    "            print(\"elapsed time for train=\", time_e - time_s, \"(s)\")\n",
    "            print(\"learning rate =\", learning_rate_value)\n",
    "            print(\"loss =\", loss_value)\n",
    "            print(\"acc =\", acc_value)\n",
    "\n",
    "            with open(\"./result.txt\", \"a\") as fd:\n",
    "                fd.write(\"%d, %f, %f, %f\\n\" % (train_iter, learning_rate_value, loss_value, acc_value))\n",
    "\n",
    "            #########################Disply Train Info.#################################\n",
    "            if train_iter % SAVE_FREQ == 0:\n",
    "                saver.save(sess, \"./model-\" + str(train_iter) + \".ckpt\")\n",
    "                print(\"model-\" + str(train_iter) + \".ckpt is saved!!\")\n",
    "        else:\n",
    "            sess.run(optimizer,\n",
    "                         feed_dict={data: images_train, label: annotations_train, do_prop: 0.5})\n",
    "            # time_e = time.time()\n",
    "            # print(\"elapsed time for train=\", time_e - time_s, \"(s)\")\n",
    "\n",
    "        if train_iter % TEST_FREQ == 0:\n",
    "            acc_test_value = 0\n",
    "            for test_iter in range(MAX_ITER_TEST):\n",
    "                if test_iter % DISP_FREQ == 0:\n",
    "                    print(\"iter test =\", test_iter)\n",
    "                images_test, annotations_test = sess.run([image_test, annotation_test])\n",
    "                images_test -= total_mean_repeat_test\n",
    "                acc_test_value += sess.run(accuracy, feed_dict={data: images_test, label: annotations_test, do_prop: 1.0})\n",
    "            acc_test_value /= MAX_ITER_TEST\n",
    "\n",
    "            print(\"*****************ACC for TEST***********************\")\n",
    "            print(\"acc for test = \", acc_test_value)\n",
    "            print(\"****************************************************\")\n",
    "            with open(\"./acc_test.txt\", \"a\") as fd_test_ret:\n",
    "                fd_test_ret.write(\"%d, %f, %f\\n\" % (train_iter, learning_rate_value, acc_test_value))\n",
    "                fd_test_ret.write(\"\\n\")\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
