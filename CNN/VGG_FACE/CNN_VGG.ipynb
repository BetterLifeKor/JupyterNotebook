{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import image_from_tfrecord as read_data\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOAD_MODEL = False\n",
    "START_ITER = 0\n",
    "model_path = \"./model-0000.ckpt\"\n",
    "\n",
    "# Define Prameters\n",
    "DATA_TYPE = tf.float32\n",
    "SEED = 4658\n",
    "\n",
    "NUM_QUEUE_THREAD = 4\n",
    "\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_VALIDATION = 25\n",
    "\n",
    "IMAGE_HEIGHT = 180\n",
    "IMAGE_WIDTH = 140\n",
    "NUM_CHANNEL = 3\n",
    "\n",
    "NUM_TRAIN_CASE = 193515\n",
    "NUM_VALIDATION_CASE = 48379\n",
    "NUM_CLASS = 8277\n",
    "\n",
    "WEIGHT_DECAY_FACTOR = 0.0005\n",
    "MOMENTUM_FACTOR = 0.9\n",
    "\n",
    "NUM_EPOCH = 30\n",
    "SAVE_FREQ = 5000\n",
    "TEST_FREQ = 1000\n",
    "DISP_FREQ = 10\n",
    "MAX_ITER = 100 # 400000\n",
    "\n",
    "MAX_ITER_TEST = NUM_VALIDATION_CASE // BATCH_SIZE_VALIDATION\n",
    "\n",
    "BASE_LR = 0.007\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=WEIGHT_DECAY_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Define Weight Variables\n",
    "# with tf.variable_scope(\"layer1\"):\n",
    "#     conv11_weight =  tf.Variable(\n",
    "#         tf.random_normal([3, 3, NUM_CHANNEL, 64],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='conv1')\n",
    "#     conv11_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[64],\n",
    "#                     dtype=DATA_TYPE), name='bias1')\n",
    "#     conv12_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 64, 64],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight2')\n",
    "#     conv12_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[64],\n",
    "#                     dtype=DATA_TYPE), name='bias2')\n",
    "# with tf.variable_scope(\"layer2\"):\n",
    "#     conv21_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 64, 128],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight1')\n",
    "#     conv21_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[128],\n",
    "#                     dtype=DATA_TYPE), name='bias1')\n",
    "#     conv22_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 128, 128],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight2')\n",
    "#     conv22_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[128],\n",
    "#                     dtype=DATA_TYPE), name='bias2')\n",
    "# with tf.variable_scope(\"layer3\"):\n",
    "#     conv31_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 128, 256],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight1')\n",
    "#     conv31_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[256],\n",
    "#                     dtype=DATA_TYPE), name='bias1')\n",
    "#     conv32_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 256, 256],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight2')\n",
    "#     conv32_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[256],\n",
    "#                     dtype=DATA_TYPE), name='bias2')\n",
    "# with tf.variable_scope(\"layer4\"):\n",
    "#     conv41_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 256, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight1')\n",
    "#     conv41_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias1')\n",
    "#     conv42_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 512, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight2')\n",
    "#     conv42_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias2')\n",
    "#     conv43_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 512, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight3')\n",
    "#     conv43_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias3')\n",
    "# with tf.variable_scope(\"layer5\"):\n",
    "#     conv51_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 512, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight1')\n",
    "#     conv51_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias1')\n",
    "#     conv52_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 512, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight2')\n",
    "#     conv52_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias2')\n",
    "#     conv53_weight = tf.Variable(\n",
    "#         tf.random_normal([3, 3, 512, 512],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight3')\n",
    "#     conv53_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[512],\n",
    "#                     dtype=DATA_TYPE), name='bias3')\n",
    "# with tf.variable_scope(\"layer6\"):\n",
    "#     fc6_weight = tf.Variable(\n",
    "#         tf.random_normal([10240, 4096],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight')\n",
    "#     fc6_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[4096],\n",
    "#                     dtype=DATA_TYPE), name='bias')\n",
    "# with tf.variable_scope(\"layer7\"):\n",
    "#     fc7_weight = tf.Variable(\n",
    "#         tf.random_normal([4096, 4096],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight')\n",
    "#     fc7_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[4096],\n",
    "#                     dtype=DATA_TYPE), name='bias')\n",
    "# with tf.variable_scope(\"layer8\"):\n",
    "#     fc8_weight = tf.Variable(\n",
    "#         tf.random_normal([4096, NUM_CLASS + 1],  # [Kernel_H, kernel_W, NUM_IN, NUM_OUT]\n",
    "#                          stddev=0.01, seed=SEED, dtype=DATA_TYPE), name='weight')\n",
    "#     fc8_bias = tf.Variable(\n",
    "#         tf.constant(0, shape=[NUM_CLASS + 1],\n",
    "#                     dtype=DATA_TYPE), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"layer1\"):\n",
    "    conv11_weight = tf.get_variable(\"weight1\", shape=[3, 3, NUM_CHANNEL, 64],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv11_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[64],\n",
    "                    dtype=DATA_TYPE), name='bias1')\n",
    "    conv12_weight = tf.get_variable(\"weight2\", shape=[3, 3, 64, 64],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv12_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[64],\n",
    "                    dtype=DATA_TYPE), name='bias2')\n",
    "with tf.variable_scope(\"layer2\"):\n",
    "    conv21_weight = tf.get_variable(\"weight1\", shape=[3, 3, 64, 128],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv21_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[128],\n",
    "                    dtype=DATA_TYPE), name='bias1')\n",
    "    conv22_weight = tf.get_variable(\"weight2\", shape=[3, 3, 128, 128],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv22_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[128],\n",
    "                    dtype=DATA_TYPE), name='bias2')\n",
    "with tf.variable_scope(\"layer3\"):\n",
    "    conv31_weight = tf.get_variable(\"weight1\", shape=[3, 3, 128, 256],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv31_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[256],\n",
    "                    dtype=DATA_TYPE), name='bias1')\n",
    "    conv32_weight = tf.get_variable(\"weight2\", shape=[3, 3, 256, 256],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv32_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[256],\n",
    "                    dtype=DATA_TYPE), name='bias2')\n",
    "with tf.variable_scope(\"layer4\"):\n",
    "    conv41_weight = tf.get_variable(\"weight1\", shape=[3, 3, 256, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv41_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias1')\n",
    "    conv42_weight = tf.get_variable(\"weight2\", shape=[3, 3, 512, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv42_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias2')\n",
    "    conv43_weight = tf.get_variable(\"weight3\", shape=[3, 3, 512, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv43_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias3')\n",
    "with tf.variable_scope(\"layer5\"):\n",
    "    conv51_weight = tf.get_variable(\"weight1\", shape=[3, 3, 512, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv51_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias1')\n",
    "    conv52_weight = tf.get_variable(\"weight2\", shape=[3, 3, 512, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED), dtype=DATA_TYPE)\n",
    "    conv52_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias2')\n",
    "    conv53_weight = tf.get_variable(\"weight3\", shape=[3, 3, 512, 512],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    conv53_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[512],\n",
    "                    dtype=DATA_TYPE), name='bias3')\n",
    "with tf.variable_scope(\"layer6\"):\n",
    "    fc6_weight = tf.get_variable(\"weight\", shape=[10240, 4096],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    fc6_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[4096],\n",
    "                    dtype=DATA_TYPE), name='bias')\n",
    "with tf.variable_scope(\"layer7\"):\n",
    "    fc7_weight = tf.get_variable(\"weight\", shape=[4096, 4096],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    fc7_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[4096],\n",
    "                    dtype=DATA_TYPE), name='bias')\n",
    "with tf.variable_scope(\"layer8\"):\n",
    "    fc8_weight = tf.get_variable(\"weight\", shape=[4096, NUM_CLASS + 1],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(seed=SEED),\n",
    "                                    regularizer=regularizer, dtype=DATA_TYPE)\n",
    "    fc8_bias = tf.Variable(\n",
    "        tf.constant(0, shape=[NUM_CLASS + 1],\n",
    "                    dtype=DATA_TYPE), name='bias')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define Deep Model\n",
    "def model(input, dropout_prop):\n",
    "    #Layer1\n",
    "    conv = tf.nn.conv2d(input, conv11_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv11_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv12_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv12_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    # Layer2\n",
    "    conv = tf.nn.conv2d(pool, conv21_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv21_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv22_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv22_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    # Layer3\n",
    "    conv = tf.nn.conv2d(pool, conv31_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv31_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv32_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv32_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    # Layer4\n",
    "    conv = tf.nn.conv2d(pool, conv41_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv41_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv42_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv42_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv43_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv43_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    # Layer5\n",
    "    conv = tf.nn.conv2d(pool, conv51_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv51_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv52_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv52_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(relu, conv53_weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, conv53_bias)\n",
    "    relu = tf.nn.relu(conv)\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    #Layer6\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    pool_reshape = tf.reshape(pool, [-1, pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "    ip = tf.matmul(pool_reshape, fc6_weight)\n",
    "    ip = ip + fc6_bias\n",
    "    relu = tf.nn.relu(ip)\n",
    "    do = tf.nn.dropout(relu, dropout_prop, seed=SEED)\n",
    "    # Layer7\n",
    "    ip = tf.matmul(do, fc7_weight)\n",
    "    ip = ip + fc7_bias\n",
    "    relu = tf.nn.relu(ip)\n",
    "    do = tf.nn.dropout(relu, dropout_prop, seed=SEED)\n",
    "    # Layer7\n",
    "    ip = tf.matmul(do, fc8_weight)\n",
    "    ip = ip + fc8_bias\n",
    "\n",
    "    return ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 180, 140, 3)\n",
      "(25, 180, 140, 3)\n"
     ]
    }
   ],
   "source": [
    "tfrecords_filename_train = '../../../DB/data/train_mirror.tfrecords'\n",
    "filename_queue_train = tf.train.string_input_producer([tfrecords_filename_train])\n",
    "image_train, annotation_train = read_data.read_and_decode(filename_queue_train, IMAGE_HEIGHT, IMAGE_WIDTH,\n",
    "                                                          NUM_CHANNEL, NUM_CLASS, BATCH_SIZE_TRAIN, NUM_QUEUE_THREAD)\n",
    "\n",
    "tfrecords_filename_test = '../../../DB/data/validation.tfrecords'\n",
    "filename_queue_test = tf.train.string_input_producer([tfrecords_filename_test])\n",
    "image_test, annotation_test = read_data.read_and_decode(filename_queue_test, IMAGE_HEIGHT, IMAGE_WIDTH,\n",
    "                                                        NUM_CHANNEL, NUM_CLASS, BATCH_SIZE_VALIDATION, NUM_QUEUE_THREAD)\n",
    "\n",
    "print(image_train.shape)\n",
    "print(image_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss\n",
    "data = tf.placeholder(\n",
    "      DATA_TYPE,\n",
    "      [None, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNEL])\n",
    "label = tf.placeholder(tf.int32, [None, NUM_CLASS + 1])\n",
    "do_prop = tf.placeholder(tf.float32)\n",
    "logit = model(data, do_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logit))\n",
    "# Weight decay term\n",
    "reg_ws = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss += tf.reduce_sum(reg_ws)\n",
    "# variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "# for variable in variables:\n",
    "#     loss += WEIGHT_DECAY_FACTOR * tf.sqrt(tf.reduce_sum(tf.square(variable)))\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "global_step = tf.Variable(0, dtype=DATA_TYPE)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.polynomial_decay(\n",
    "    learning_rate=BASE_LR,\n",
    "    global_step=global_step,\n",
    "    decay_steps=MAX_ITER * 2,\n",
    "    end_learning_rate=0.0,\n",
    "    power=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 180, 140, 3)\n",
      "(25, 180, 140, 3)\n"
     ]
    }
   ],
   "source": [
    "# Train operator\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=MOMENTUM_FACTOR)\\\n",
    "    .minimize(loss, global_step=global_step)\n",
    "\n",
    "# Calculate accuracy\n",
    "prediction = tf.argmax(tf.nn.softmax(logit), 1)\n",
    "train_label = tf.argmax(label, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, train_label), tf.float32))\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "# init_op = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "total_mean = np.zeros([1, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNEL], dtype=np.float32)\n",
    "with open(\"image_mean.txt\", \"r\") as fd:\n",
    "    image_list = fd.read()\n",
    "    image_list = image_list.split(\"\\n\")\n",
    "    idx = 0\n",
    "    for i in range(IMAGE_HEIGHT):\n",
    "        for j in range(IMAGE_WIDTH):\n",
    "            for k in range(NUM_CHANNEL):\n",
    "                total_mean[0][i][j][k] = image_list[idx]\n",
    "                idx += 1\n",
    "total_mean_repeat_train = np.repeat(total_mean, BATCH_SIZE_TRAIN, axis=0)\n",
    "total_mean_repeat_test = np.repeat(total_mean, BATCH_SIZE_VALIDATION, axis=0)\n",
    "print(total_mean_repeat_train.shape)\n",
    "print(total_mean_repeat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "elapsed time for train= 9.339600563049316 (s)\n",
      "learning rate = 0.007\n",
      "loss = 13.6799\n",
      "acc = 0.0\n",
      "model-0.ckpt is saved!!\n",
      "iter test = 0\n",
      "iter test = 10\n",
      "iter test = 20\n",
      "iter test = 30\n",
      "iter test = 40\n",
      "iter test = 50\n",
      "iter test = 60\n",
      "iter test = 70\n",
      "iter test = 80\n",
      "iter test = 90\n",
      "iter test = 100\n",
      "iter test = 110\n",
      "iter test = 120\n",
      "iter test = 130\n",
      "iter test = 140\n",
      "iter test = 150\n",
      "iter test = 160\n",
      "iter test = 170\n",
      "iter test = 180\n",
      "iter test = 190\n",
      "iter test = 200\n",
      "iter test = 210\n",
      "iter test = 220\n",
      "iter test = 230\n",
      "iter test = 240\n",
      "iter test = 250\n",
      "iter test = 260\n",
      "iter test = 270\n",
      "iter test = 280\n",
      "iter test = 290\n",
      "iter test = 300\n",
      "iter test = 310\n",
      "iter test = 320\n",
      "iter test = 330\n",
      "iter test = 340\n",
      "iter test = 350\n",
      "iter test = 360\n",
      "iter test = 370\n",
      "iter test = 380\n",
      "iter test = 390\n",
      "iter test = 400\n",
      "iter test = 410\n",
      "iter test = 420\n",
      "iter test = 430\n",
      "iter test = 440\n",
      "iter test = 450\n",
      "iter test = 460\n",
      "iter test = 470\n",
      "iter test = 480\n",
      "iter test = 490\n",
      "iter test = 500\n",
      "iter test = 510\n",
      "iter test = 520\n",
      "iter test = 530\n",
      "iter test = 540\n",
      "iter test = 550\n",
      "iter test = 560\n",
      "iter test = 570\n",
      "iter test = 580\n",
      "iter test = 590\n",
      "iter test = 600\n",
      "iter test = 610\n",
      "iter test = 620\n",
      "iter test = 630\n",
      "iter test = 640\n",
      "iter test = 650\n",
      "iter test = 660\n",
      "iter test = 670\n",
      "iter test = 680\n",
      "iter test = 690\n",
      "iter test = 700\n",
      "iter test = 710\n",
      "iter test = 720\n",
      "iter test = 730\n",
      "iter test = 740\n",
      "iter test = 750\n",
      "iter test = 760\n",
      "iter test = 770\n",
      "iter test = 780\n",
      "iter test = 790\n",
      "iter test = 800\n",
      "iter test = 810\n",
      "iter test = 820\n",
      "iter test = 830\n",
      "iter test = 840\n",
      "iter test = 850\n",
      "iter test = 860\n",
      "iter test = 870\n",
      "iter test = 880\n",
      "iter test = 890\n",
      "iter test = 900\n",
      "iter test = 910\n",
      "iter test = 920\n",
      "iter test = 930\n",
      "iter test = 940\n",
      "iter test = 950\n",
      "iter test = 960\n",
      "iter test = 970\n",
      "iter test = 980\n",
      "iter test = 990\n",
      "iter test = 1000\n",
      "iter test = 1010\n",
      "iter test = 1020\n",
      "iter test = 1030\n",
      "iter test = 1040\n",
      "iter test = 1050\n",
      "iter test = 1060\n",
      "iter test = 1070\n",
      "iter test = 1080\n",
      "iter test = 1090\n",
      "iter test = 1100\n",
      "iter test = 1110\n",
      "iter test = 1120\n",
      "iter test = 1130\n",
      "iter test = 1140\n",
      "iter test = 1150\n",
      "iter test = 1160\n",
      "iter test = 1170\n",
      "iter test = 1180\n",
      "iter test = 1190\n",
      "iter test = 1200\n",
      "iter test = 1210\n",
      "iter test = 1220\n",
      "iter test = 1230\n",
      "iter test = 1240\n",
      "iter test = 1250\n",
      "iter test = 1260\n",
      "iter test = 1270\n",
      "iter test = 1280\n",
      "iter test = 1290\n",
      "iter test = 1300\n",
      "iter test = 1310\n",
      "iter test = 1320\n",
      "iter test = 1330\n",
      "iter test = 1340\n",
      "iter test = 1350\n",
      "iter test = 1360\n",
      "iter test = 1370\n",
      "iter test = 1380\n",
      "iter test = 1390\n",
      "iter test = 1400\n",
      "iter test = 1410\n",
      "iter test = 1420\n",
      "iter test = 1430\n",
      "iter test = 1440\n",
      "iter test = 1450\n",
      "iter test = 1460\n",
      "iter test = 1470\n",
      "iter test = 1480\n",
      "iter test = 1490\n",
      "iter test = 1500\n",
      "iter test = 1510\n",
      "iter test = 1520\n",
      "iter test = 1530\n",
      "iter test = 1540\n",
      "iter test = 1550\n",
      "iter test = 1560\n",
      "iter test = 1570\n",
      "iter test = 1580\n",
      "iter test = 1590\n",
      "iter test = 1600\n",
      "iter test = 1610\n",
      "iter test = 1620\n",
      "iter test = 1630\n",
      "iter test = 1640\n",
      "iter test = 1650\n",
      "iter test = 1660\n",
      "iter test = 1670\n",
      "iter test = 1680\n",
      "iter test = 1690\n",
      "iter test = 1700\n",
      "iter test = 1710\n",
      "iter test = 1720\n",
      "iter test = 1730\n",
      "iter test = 1740\n",
      "iter test = 1750\n",
      "iter test = 1760\n",
      "iter test = 1770\n",
      "iter test = 1780\n",
      "iter test = 1790\n",
      "iter test = 1800\n",
      "iter test = 1810\n",
      "iter test = 1820\n",
      "iter test = 1830\n",
      "iter test = 1840\n",
      "iter test = 1850\n",
      "iter test = 1860\n",
      "iter test = 1870\n",
      "iter test = 1880\n",
      "iter test = 1890\n",
      "iter test = 1900\n",
      "iter test = 1910\n",
      "iter test = 1920\n",
      "iter test = 1930\n",
      "*****************ACC for TEST***********************\n",
      "acc for test =  0.000103359170816\n",
      "****************************************************\n",
      "iter 10\n",
      "elapsed time for train= 0.6233735084533691 (s)\n",
      "learning rate = 0.00665\n",
      "loss = 13.6537\n",
      "acc = 0.0\n",
      "iter 20\n",
      "elapsed time for train= 0.6233737468719482 (s)\n",
      "learning rate = 0.0063\n",
      "loss = 13.6805\n",
      "acc = 0.0\n",
      "iter 30\n",
      "elapsed time for train= 0.6433858871459961 (s)\n",
      "learning rate = 0.00595\n",
      "loss = 13.6392\n",
      "acc = 0.0\n",
      "iter 40\n",
      "elapsed time for train= 0.6353809833526611 (s)\n",
      "learning rate = 0.0056\n",
      "loss = 13.645\n",
      "acc = 0.0\n",
      "iter 50\n",
      "elapsed time for train= 0.630378246307373 (s)\n",
      "learning rate = 0.00525\n",
      "loss = 13.6414\n",
      "acc = 0.0\n",
      "iter 60\n",
      "elapsed time for train= 0.630378007888794 (s)\n",
      "learning rate = 0.0049\n",
      "loss = 13.6433\n",
      "acc = 0.0\n",
      "iter 70\n",
      "elapsed time for train= 0.626375675201416 (s)\n",
      "learning rate = 0.00455\n",
      "loss = 13.6625\n",
      "acc = 0.0\n",
      "iter 80\n",
      "elapsed time for train= 0.6323792934417725 (s)\n",
      "learning rate = 0.0042\n",
      "loss = 13.6342\n",
      "acc = 0.0\n",
      "iter 90\n",
      "elapsed time for train= 0.6203720569610596 (s)\n",
      "learning rate = 0.00385\n",
      "loss = 13.6272\n",
      "acc = 0.0\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        saver.restore(sess, model_path)\n",
    "    else:\n",
    "        sess.run(init_op)\n",
    "\n",
    "    for train_iter in range(START_ITER, MAX_ITER):\n",
    "        # time_s = time.time()\n",
    "        images_train, annotations_train = sess.run([image_train, annotation_train])\n",
    "        images_train -= total_mean_repeat_train\n",
    "        # time_e = time.time()\n",
    "        # print(\"elapsed time for load image =\", time_e - time_s, \"(s)\")\n",
    "\n",
    "        time_s = time.time()\n",
    "        if train_iter % DISP_FREQ == 0:\n",
    "            #########################Disply Train Info.#################################\n",
    "            print(\"iter\", train_iter)\n",
    "            _, loss_value, acc_value, learning_rate_value = \\\n",
    "                sess.run([optimizer, loss, accuracy, learning_rate],\n",
    "                         feed_dict={data: images_train, label: annotations_train, do_prop: 0.5})\n",
    "            time_e = time.time()\n",
    "\n",
    "            # writer.add_summary(summary, global_step=train_iter)\n",
    "\n",
    "            print(\"elapsed time for train=\", time_e - time_s, \"(s)\")\n",
    "            print(\"learning rate =\", learning_rate_value)\n",
    "            print(\"loss =\", loss_value)\n",
    "            print(\"acc =\", acc_value)\n",
    "\n",
    "            with open(\"./result.txt\", \"a\") as fd:\n",
    "                fd.write(\"%d, %f, %f, %f\\n\" % (train_iter, learning_rate_value, loss_value, acc_value))\n",
    "\n",
    "            #########################Disply Train Info.#################################\n",
    "            if train_iter % SAVE_FREQ == 0:\n",
    "                saver.save(sess, \"./model-\" + str(train_iter) + \".ckpt\")\n",
    "                print(\"model-\" + str(train_iter) + \".ckpt is saved!!\")\n",
    "        else:\n",
    "            sess.run(optimizer,\n",
    "                         feed_dict={data: images_train, label: annotations_train, do_prop: 0.5})\n",
    "            # time_e = time.time()\n",
    "            # print(\"elapsed time for train=\", time_e - time_s, \"(s)\")\n",
    "\n",
    "        if train_iter % TEST_FREQ == 0:\n",
    "            acc_test_value = 0\n",
    "            for test_iter in range(MAX_ITER_TEST):\n",
    "                if test_iter % DISP_FREQ == 0:\n",
    "                    print(\"iter test =\", test_iter)\n",
    "                images_test, annotations_test = sess.run([image_test, annotation_test])\n",
    "                images_test -= total_mean_repeat_test\n",
    "                acc_test_value += sess.run(accuracy, feed_dict={data: images_test, label: annotations_test, do_prop: 1.0})\n",
    "            acc_test_value /= MAX_ITER_TEST\n",
    "\n",
    "            print(\"*****************ACC for TEST***********************\")\n",
    "            print(\"acc for test = \", acc_test_value)\n",
    "            print(\"****************************************************\")\n",
    "            with open(\"./acc_test.txt\", \"a\") as fd_test_ret:\n",
    "                fd_test_ret.write(\"%d, %f, %f\\n\" % (train_iter, learning_rate_value, acc_test_value))\n",
    "                fd_test_ret.write(\"\\n\")\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "fd.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
